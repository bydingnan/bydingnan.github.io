
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>
    
  HDFS集群配置、搭建、备份 - 
  

  </title>
  <meta name="author" content="">
  <meta name="description" content="">

  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link href="asset/css/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="atom.xml" rel="alternate" title="" type="application/atom+xml">
  <script src="asset/js/modernizr-2.0.js"></script>
  <script src="asset/js/jquery.min.js"></script>
  <script src="asset/highlightjs/highlight.pack.js"></script>
  <link href="asset/highlightjs/styles/solarized_light.css" media="screen, projection" rel="stylesheet" type="text/css">
  <script>hljs.initHighlightingOnLoad();</script>

  <style type="text/css">
  .cat-children-p{ padding: 6px 0px;}
  .hljs{background: none;}
  </style>
  <script type="text/javascript">
  var isAddSildbar = true;
  </script>
  <script src="asset/js/octopress.js" type="text/javascript"></script>
</head>
<script type="text/javascript">
//链接新开窗口
function addBlankTargetForLinks () {
  $('a[href^="http"]').each(function(){
      $(this).attr('target', '_blank');
  });
}
$(document).ready(function(event) {
  addBlankTargetForLinks();
});
</script>
<body   >
  <header role="banner"><hgroup>
  <h1><a href="index.html"></a></h1>
  
    <h2></h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="http://google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="index.html">Home</a></li>
  <li><a href="archives.html">Archives</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content"> 
<div>
	<article class="hentry" role="article">
	<header>
			  	<h1 class="entry-title">HDFS集群配置、搭建、备份</h1>
				<p class="meta"><time datetime="2016-08-01T15:29:32+08:00" pubdate data-updated="true">2016/8/1</time></p>
			 </header>
		  	<div class="entry-content">
			  	<p>以实际情况为例，三台机器组成的集群， 系统为Ubuntu 16.04 server版</p>

<h2 id="toc_0">1 环境配置</h2>

<p>搭建了一个三台机器的集群：</p>

<pre><code class="language-shell">10.0.1.215  NameNode      talkmate  Talkmate123
10.0.1.217  DataNode1     talkmate  Talkmate123
10.0.1.219  DataNode2     talkmate  Talkmate123
</code></pre>

<p>以上三列分别为IP hostname 用户名 密码</p>

<h3 id="toc_1">1.1 修改hostname</h3>

<p>　　Hostname在<strong><code>/etc/hostname</code></strong>中修改，域名映射关系在<strong><code>/etc/hosts</code></strong>中修改，对于三台机器都需要修改。</p>

<h3 id="toc_2">1.2 打通NameNode到DataNode节点的SSH无密码登陆</h3>

<p>　　一般系统是默认安装了ssh命令的。如果没有，或者版本比较老，则可以重新安装：<br/>
 <code>sudo apt-get install ssh</code></p>

<pre><code class="language-shell">  cd ~/.ssh
  ssh-keygen –t rsa
  cat id_rsa.pub &gt; authorized_keys
</code></pre>

<p>　　至此可以测试一下ssh 本机IP是否需要密码，如果成功，说明本机配置成功。</p>

<h3 id="toc_3">1.3 将authorized_keys文件复制到所有DataNode节点</h3>

<pre><code class="language-shell">scp authorized_keys talkmate@DataNode1:/home/talkmate/.ssh/
scp authorized_keys talkmate@DataNode2:/home/talkmate/.ssh/
</code></pre>

<p>　　至此免密码登录配置完毕，可以通过本机ssh各个节点IP来测试是否需要密码登录。</p>

<h3 id="toc_4">1.4 下载Hadoop软件包，官方下载地址</h3>

<p><a href="http://mirrors.cnnic.cn/apache/hadoop/common/stable/">http://mirrors.cnnic.cn/apache/hadoop/common/stable/</a><br/><br/>
<a href="http://hadoop.apache.org/releases.html#Download">http://hadoop.apache.org/releases.html#Download</a></p>

<pre><code class="language-shell">tar zxvf hadoop-2.6.4.tar.gz
mv hadoop-2.6.4 /home/talkmate/softwares
</code></pre>

<h3 id="toc_5">1.5 配置环境变量</h3>

<pre><code class="language-shell">vim ~/.bash_profile

export JAVA_HOME=/home/talkmate/softwares/jdk1.8.0_91
export HADOOP_HOME=/home/talkmate/softwares/hadoop-2.6.4
export PATH=~/softwares/usr/bin:$JAVA_HOME/bin:$HADOOP_HOME/bin:$PATH
</code></pre>

<h2 id="toc_6">2 Hadoop配置文件</h2>

<h3 id="toc_7">2.1 相关文件</h3>

<p>在NameNode上修改hadoop配置文件<br/>
主要修改<strong><code>/home/talkmate/softwares/hadoop-2.6.4/etc/hadoop/</code></strong>目录下的配置文件</p>

<p><strong><code>hadoop-env.sh</code></strong>　　　Hadoop环境变量设置<br/><br/>
<strong><code>core-site.xml</code></strong>　　　NameNode、IP和端口设置、全局设置等<br/><br/>
<strong><code>hdfs-site.xml</code></strong>　　　HDFS数据块副本等具体参数<br/><br/>
<strong><code>mapred-site.xml</code></strong>　　MapReduce、jobhistory、框架设置<br/><br/>
<strong><code>yarn-env.sh</code></strong>　　　　Yarn环境变量设置<br/><br/>
<strong><code>yarn-site.xml</code></strong>　　　Yarn框架具体设置<br/><br/>
<strong><code>masters</code></strong>　　　　　　　主节点<br/><br/>
<strong><code>slaves</code></strong>　　　　　　　　从节点</p>

<h3 id="toc_8">2.2 core-site.xml</h3>

<pre><code class="language-xml">&lt;configuration&gt;
    &lt;property&gt;
        &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;
        &lt;value&gt;/home/talkmate/hdfs_data/tmp&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;fs.defaultFS&lt;/name&gt;
        &lt;value&gt;hdfs://NameNode:8020&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;io.file.buffer.size&lt;/name&gt;
        &lt;value&gt;131072&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;fs.trash.interval&lt;/name&gt;
        &lt;value&gt;10080&lt;/value&gt;
    &lt;/property&gt;
&lt;/configuration&gt;
</code></pre>

<h3 id="toc_9">2.3 hadoop-env.sh</h3>

<ul>
<li>将配置文件中的<strong><code>export HADOOP_CONF_DIR=${HADOOP_CONF_DIR:-&quot;/etc/hadoop&quot;}</code></strong>改为<br/>
<strong><code>export HADOOP_CONF_DIR=/home/talkmate/softwares/hadoop-2.6.4/etc/hadoop</code></strong><br/></li>
<li>增加JAVA_HOME<br/>
<strong><code>export JAVA_HOME=/home/talkmate/softwares/jdk1.8.0_91</code></strong></li>
</ul>

<h3 id="toc_10">2.4 yarn-env.sh</h3>

<ul>
<li>将配置文件中的<strong><code>export YARN_CONF_DIR=&quot;${YARN_CONF_DIR:-$HADOOP_YARN_HOME/conf}&quot;</code></strong>改为<br/>
<strong><code>export YARN_CONF_DIR=/home/talkmate/softwares/hadoop-2.6.4/etc/hadoop</code></strong><br/></li>
<li>增加JAVA_HOME<br/>
<strong><code>export JAVA_HOME=/home/talkmate/softwares/jdk1.8.0_91</code></strong></li>
</ul>

<h3 id="toc_11">2.5 hdfs-site.xml</h3>

<pre><code class="language-xml">&lt;configuration&gt;
    &lt;property&gt;
        &lt;name&gt;dfs.replication&lt;/name&gt;
        &lt;value&gt;3&lt;/value&gt;                                                                                                      
    &lt;/property&gt;                                                                                                          
    &lt;property&gt;
        &lt;name&gt;dfs.name.dir&lt;/name&gt;
        &lt;value&gt;/home/talkmate/hdfs_data/name&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;dfs.data.dir&lt;/name&gt;
        &lt;value&gt;/home/talkmate/hdfs_data/data&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt;
        &lt;value&gt;true&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;dfs.namenode.checkpoint.period&lt;/name&gt;
        &lt;value&gt;3600&lt;/value&gt;
        &lt;description&gt;The number of seconds between two periodic checkpoints.  &lt;/description&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;fs.checkpoint.size&lt;/name&gt;
        &lt;value&gt;67108864&lt;/value&gt;
        &lt;description&gt;The size of the current edit log (in bytes) that triggers a periodic checkpoint even if the fs.checkpoint.period hasn&#39;t expired.  
        &lt;/description&gt;
    &lt;/property&gt;

    &lt;property&gt;
        &lt;name&gt;dfs.namenode.checkpoint.dir&lt;/name&gt;
        &lt;value&gt;/home/talkmate/hdfs_data/checkpoint&lt;/value&gt;
        &lt;description&gt;Determines where on the local filesystem the DFS secondary name node should store the temporary images to merge. If this is a comma-delimited list of directories then the image is replicated in all of the directories for redundancy.  &lt;/description&gt;
    &lt;/property&gt;

    &lt;property&gt;
        &lt;name&gt;dfs.http.address&lt;/name&gt;
        &lt;value&gt;NameNode:50070&lt;/value&gt;
        &lt;description&gt;
            The address and the base port where the dfs namenode web ui will listen on.
            If the port is 0 then the server will start on a free port.
        &lt;/description&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;dfs.secondary.http.address&lt;/name&gt;
        &lt;value&gt;DataNode1:50090&lt;/value&gt;
        &lt;description&gt;NameNode get the newest fsimage via dfs.secondary.http.address&lt;/description&gt;
    &lt;/property&gt;
&lt;/configuration&gt;
</code></pre>

<h3 id="toc_12">2.6 mapred-site.xml</h3>

<pre><code class="language-xml">&lt;configuration&gt;
    &lt;property&gt;
        &lt;name&gt;mapreduce.framework.name&lt;/name&gt;
        &lt;value&gt;yarn&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;
        &lt;value&gt;NameNode:10020&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;
        &lt;value&gt;NameNode:19888&lt;/value&gt;
    &lt;/property&gt;
&lt;/configuration&gt;
</code></pre>

<h3 id="toc_13">2.7 yarn-site.xml</h3>

<pre><code class="language-xml">&lt;configuration&gt;
    &lt;property&gt;
        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;
        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt;
        &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt;
        &lt;value&gt;NameNode:8032&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt;
        &lt;value&gt;NameNode:8030&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt;
        &lt;value&gt;NameNode:8031&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;yarn.resourcemanager.admin.address&lt;/name&gt;
        &lt;value&gt;NameNode:8033&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt;
        &lt;value&gt;NameNode:8088&lt;/value&gt;
    &lt;/property&gt;
&lt;/configuration&gt;
</code></pre>

<h3 id="toc_14">2.8 masters</h3>

<pre><code class="language-shell">NameNode
</code></pre>

<h3 id="toc_15">2.9 slaves</h3>

<pre><code class="language-shell">DataNode1
DataNode2
</code></pre>

<h2 id="toc_16">3 发布</h2>

<h2 id="toc_17">3.1 将NameNode节点配置好的Hadoop目录复制到每一个DataNode节点上</h2>

<p><strong><code>scp –r /home/talkmate/softwares/hadoop-2.6.4 DataNode1@:/home/talkmate/softwares/</code></strong><br/><br/>
<strong><code>scp –r /home/talkmate/softwares/hadoop-2.6.4 DataNode1@:/home/talkmate/softwares/</code></strong></p>

<h3 id="toc_18">3.2 在NameNode节点上格式化NameNode</h3>

<pre><code class="language-shell">hadoop namenode –format
</code></pre>

<h3 id="toc_19">3.3 执行</h3>

<p>运行</p>

<pre><code class="language-shel">sbin/start-all.sh
</code></pre>

<p>停止</p>

<pre><code class="language-shell">sbin/stop-all.sh
</code></pre>

<h3 id="toc_20">3.4 查看Hadoop工作情况</h3>

<p><strong><code>http://namenode:50070/</code></strong> :  主页<br/><br/>
<strong><code>http://namenode:8088/</code></strong>  :   jobs追踪页面<br/><br/>
<strong><code>hadoop dfsadmin -report</code></strong> : 集群状态<br/><br/>
<strong><code>jps</code></strong> : 节点工作情况，按目前的配置状况，正常情况应该是：</p>

<p>NameNode   </p>

<pre><code class="language-shell">16656 NameNode
17347 Application
21546 Jps
10491 JobHistoryServer
3870 SecondaryNameNode
15727 ResourceManager
</code></pre>

<p>DataNode1</p>

<pre><code class="language-shell">6706 Jps
32658 DataNode
437 NodeManager
319 SecondaryNameNode
</code></pre>

<p>DataNode2</p>

<pre><code class="language-shell">2675 NodeManager
21124 Jps
2543 DataNode
</code></pre>

<h2 id="toc_21">4 备份</h2>

<h3 id="toc_22">4.1 相关配置</h3>

<p>已经在配置文件中配置了相关字段:<br/><br/>
<strong><code>dfs.replication</code></strong> : 每个记录保存3份<br/><br/>
<strong><code>dfs.namenode.checkpoint.dir</code></strong>: checkpoint保存文件夹，恢复数据用<br/><br/>
<strong><code>dfs.secondary.http.address</code></strong> :  secondary节点位置</p>

<h3 id="toc_23">4.2 恢复：</h3>

<p>　　若是DataNode丢失，hadoop在数据被再次使用时进行检查，如果发现少了一个备份，会进行数据恢复工作。也就是说，数据恢复是自动，这也是hadoop的强大之处。  </p>

<ul>
<li>至于NameNode的恢复，在非NameNode节点上，保存一个日志镜像secondaryNnameNode。若NameNode出现问题，将secondary namenode 存储的checkpoint数据copy到NameNode上，进行导入恢复。<br/></li>
</ul>

<pre><code class="language-shell">scp -r talkmate@DataNode1:/home/talkmate/hdfs_data/checkpoint/ .
</code></pre>

<ul>
<li>kill掉DataNode进程</li>
</ul>

<pre><code class="language-shell">jps
</code></pre>

<ul>
<li>删除name目录下的所有内容，但是必须保证name这个目录是存在的，然后进行恢复</li>
</ul>

<pre><code class="language-shell">hadoop namenode -importCheckpoint  
</code></pre>

<p>正常启动以后，屏幕上会显示很多log，这个时候namenode就可以正常访问了 </p>

<ul>
<li>检查文件Block的完整性 </li>
</ul>

<pre><code class="language-shell">hadoop fsck /

</code></pre>

<ul>
<li><p>如果是HEALTHY，停止namenode，使用crrl+C或者会话结束 </p></li>
<li><p>down掉的节点经过恢复后，可以直接链接进入hadoop集群，而不用重新启动集群。命令是  </p></li>
</ul>

<pre><code class="language-shell">sbin/hadoop-daemon.sh start namenode
</code></pre>

			</div>

		
	  
		<footer>
		 <p class="meta">

			<strong>Categories:</strong>&nbsp; 
			<span class="categories">
			
			    <a class='category' href='%E6%97%A5%E5%BF%97.html'>日志</a>&nbsp;
			 
			</span>
		    </p>
		    <p class="meta">
		      
		 </p>
	    
		<div class="sharing">
			
		</div>

	    <p class="meta">
	    
	    
	        <a class="basic-alignment right" href="14700365205842.html" 
	        title="Next Post: Flume配置方案、源码修改">Flume配置方案、源码修改 &raquo;</a>
	    
	    </p>
	  </footer>
	</article>
</div>
 <aside class="sidebar"> 

	<section>
	  <h1>Categories</h1>
	  <ul id="recent_posts">
	  
	      <li class="post">
	        <a href="%E7%AC%94%E8%AE%B0.html"><strong>笔记&nbsp;(3)</strong></a>
	        
	        
	        
	      </li>
	  
	      <li class="post">
	        <a href="%E6%97%A5%E5%BF%97.html"><strong>日志&nbsp;(3)</strong></a>
	        
	        
	        
	      </li>
	   
	  </ul>
	</section>
	<section>
	  <h1>Recent Posts</h1>
	  <ul id="recent_posts">
	  
	      
		      <li class="post">
		        <a href="14700365720801.html">HDFS集群配置、搭建、备份</a>
		      </li>
	     
	  
	      
		      <li class="post">
		        <a href="14700365205842.html">Flume配置方案、源码修改</a>
		      </li>
	     
	  
	      
		      <li class="post">
		        <a href="14700364668003.html">基于HTK的孤立词无监督识别</a>
		      </li>
	     
	  
	      
		      <li class="post">
		        <a href="14700359952396.html">HDFS数据表Schema</a>
		      </li>
	     
	  
	      
		      <li class="post">
		        <a href="14700356814047.html">基于HTK的孤立词有监督识别</a>
		      </li>
	     
	  
	      
	   
	  </ul>
	</section>
	
</aside> </div></div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2014 -  -
  <span class="credit">Powered by <a target="_blank" href="http://www.mweb.im">MWeb</a> &nbsp;&nbsp; Theme by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>



</body>
</html>